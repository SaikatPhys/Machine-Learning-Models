# Fine tuning of various BERT-like transformer models

**Description:** Various pre-trained language models are fine-tuned and tested on publicly available datasets.

**Contents**

* [Fine tuning of Hindi-BERT with **Keras** using a review dataset](https://github.com/SaikatPhys/Machine-Learning-Models/blob/main/Hindi-BERT-fine-tuning-with-keras-using-review-dataset.ipynb)

* [Fine tuning of T-XLM-RoBERTa-base model using transformers **Trainer class** on UMSAB Hindi sentiment analysis dataset](https://github.com/SaikatPhys/Machine-Learning-Models/blob/main/T-XLM-RoBERTa-base-fine-tuning-for-sentiment-analysis-task-using-UMSAB-dataset.ipynb)

* [A general fine-tuning recipe of (BERT like) Hugging Face transformer models using **native PyTorch** (training workflow is demonstrated on German UMSAB dataset)](https://github.com/SaikatPhys/NLP-Transformer-Models/blob/main/finetune-transformers-with-pytorch.ipynb) 

* [A general hyperparameter optimization recipe for BERT-like `Hugging face` transformer models by using the transformers **Trainer** class.](https://github.com/SaikatPhys/nlp-transformer-models/blob/main/hyperparameter-optimization-of-tranformers.ipynb)
